# -*- coding: utf-8 -*-
"""AIT636_Assignment10B.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sTEuWZXncnhA09zWrDtSAk9ZyRnzmEAb
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier

# Loading dataset
dataset = pd.read_csv('/content/drive/My Drive/AIT_636_3rd sem/pima-indians-diabetes.csv', index_col=0)
feature_names = dataset.columns[:-1]

# Preprocessing the data
scaler = StandardScaler()
scaled_features = scaler.fit_transform(df.drop('target', axis=1))
df_feat = pd.DataFrame(scaled_features, columns=df.columns[:-1])
x_train, x_test, y_train, y_test = train_test_split(scaled_features, dataset['target'], test_size=0.3, stratify=dataset['target'], random_state=42)

baseline_clf = MLPClassifier(random_state=1, # Pass an int for reproducible results across multiple function calls.
                    hidden_layer_sizes=(20, 20), # tuple, length = n_layers - 2, default=(100,). The ith element represents the number of neurons in the ith hidden layer.
                    activation='relu', # {'identity', 'logistic', 'tanh', 'relu'}, default='relu'. Activation function for the hidden layer.
                    solver='adam', # {'lbfgs', 'sgd', 'dam'}, default='adam'. The solver for weight optimization.
                    alpha=0.00001, # L2 penalty (regularization term) parameter.
                    batch_size='auto', # int, default='auto'. Size of minibatches for stochastic optimizers. When set to 'auto', batch_size=min(200, n_samples).
                    learning_rate='adaptive', # {'constant', 'invscaling', 'adaptive'}, default='constant'. Learning rate schedule for weight updates.
                    # 'constant' is a constant learning rate given by 'learning_rate_init'.
                    # 'invscaling' gradually decreases the learning rate at each time step 't' using an inverse scaling exponent of 'power_t'. effective_learning_rate = learning_rate_init / pow(t, power_t)
                    # 'adaptive' keeps the learning rate constant to 'learning_rate_init' as long as training loss keeps decreasing. Each time two consecutive epochs fail to decrease training loss by at least tol, or fail to increase validation score by at least tol if 'early_stopping' is on, the current learning rate is divided by 5.
                    learning_rate_init=0.001, # The initial learning rate used. It controls the step-size in updating the weights.
                    max_iter=1000, # Maximum number of iterations. The solver iterates until convergence (determined by 'tol') or this number of iterations.
                    shuffle=True, # Whether to shuffle samples in each iteration.
                    tol=0.0001, # default=1e-4. Tolerance for the optimization.
                    # When the loss or score is not improving by at least tol for n_iter_no_change consecutive iterations, convergence is considered to be reached and training stops.
                    early_stopping=False, # Whether to use early stopping to terminate training when validation score is not improving.
                    # If set to true, it will automatically set aside 10% of training data as validation and terminate training when validation score is not improving by at least tol for n_iter_no_change consecutive epochs.
                    n_iter_no_change=10) # Maximum number of epochs to not meet tol improvement. Only effective when solver='sgd' or 'adam'.

# Fitting the model and calculating the loss curve
baseline_clf.fit(x_train, y_train)

# Print the number of iterations run and the loss curve
print("Number of iterations:", baseline_clf.n_iter_)
print("Loss Curve:", baseline_clf.loss_curve_)

# Plot the loss curve
plt.figure(figsize=(10, 6))
plt.plot(range(len(baseline_clf.loss_curve_)), baseline_clf.loss_curve_, color='blue', linestyle='solid', marker='.', markerfacecolor='red', markersize=1)
plt.title('Loss vs. Iteration')
plt.xlabel('Iteration')
plt.ylabel('Loss')
plt.show()

# Configurations for Cross-Validation
activation_values = ['relu', 'identity', 'logistic', 'tanh']
hidden_layer_values = [(10,), (20,), (10, 10), (20, 20)]

# Dictionary to store cross-validated metrics for each configuration
cv_results = {activation: [] for activation in activation_values}

for activation in activation_values:
    for hidden_layer in hidden_layer_values:
        # Define and train the model with each configuration
        clf = MLPClassifier(
            random_state=1,
            hidden_layer_sizes=hidden_layer,
            activation=activation,
            solver='adam',
            alpha=0.00001,
            batch_size='auto',
            learning_rate='adaptive',
            learning_rate_init=0.001,
            max_iter=1000,
            shuffle=True,
            tol=0.0001,
            early_stopping=True,
            n_iter_no_change=10
        )

        # Cross-validation to calculate metrics
        accuracy = cross_val_score(clf, scaled_features, dataset['target'], cv=10, scoring='accuracy').mean()
        precision = cross_val_score(clf, scaled_features, dataset['target'], cv=10, scoring='precision').mean()
        recall = cross_val_score(clf, scaled_features, dataset['target'], cv=10, scoring='recall').mean()
        f1_score = cross_val_score(clf, scaled_features, dataset['target'], cv=10, scoring='f1').mean()

        # Store the cross-validated results
        cv_results[activation].append({
            'hidden_layer_sizes': hidden_layer,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1_score
        })

        # Print each configuration's metrics
        print(f"{activation} - Hidden Layers: {hidden_layer}, "
              f"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1_score:.4f}")

# Generating a Plot for Accuracy vs. Hidden Layer Sizes for Each Activation Function
hidden_layer_labels = ['(10,)', '(20,)', '(10, 10)', '(20, 20)']

plt.figure(figsize=(10, 6))

for activation in activation_values:
    accuracies = [config['accuracy'] for config in cv_results[activation]]
    plt.plot(hidden_layer_labels, accuracies, marker='o', label=activation)

plt.title('Accuracy vs Hidden Layer Size')
plt.xlabel('Hidden Layer Size')
plt.ylabel('Accuracy')
plt.legend(title='Activation Function')
plt.grid(True)
plt.show()