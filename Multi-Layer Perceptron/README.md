# ğŸ§  Learning in Layers: Multi-Layer Perceptron Classifier

This project demonstrates the implementation of a **Multi-Layer Perceptron (MLP)** â€” a type of feedforward artificial neural network â€” for classification.

ğŸ§ª This project uses a simple MLP model to classify structured data based on multiple input features.

---

## ğŸ§  What is a Multi-Layer Perceptron?

An MLP consists of an input layer, one or more **hidden layers**, and an output layer. Each layer contains neurons that apply weighted sums and activation functions to learn patterns in data.

MLPs are the foundation of:
- Deep learning
- Neural networks used in image/text classification
- Recommender systems and more

---

## ğŸš€ What This Project Covers

- Implementing MLP with scikit-learn's `MLPClassifier`
- Normalizing input features
- Training and testing the model
- Evaluating performance using:
  - Accuracy
  - Precision, Recall, F1 Score
  - Confusion matrix
- Analyzing convergence behavior and learning curves

---

## ğŸ“Š Results Summary

> âœ… Achieved strong classification performance using ReLU activation  
> âœ… Model trained successfully with tuned hyperparameters (hidden layer size, alpha)  
> âœ… Balanced classification metrics and fast convergence

---

## ğŸ—‚ï¸ Files Included

- `MLP.py` â€” MLP implementation using `MLPClassifier`  
- `MLP_Results.pdf` â€” Report with results and discussion

---

## ğŸ“š Technologies & Libraries

```bash
scikit-learn
numpy
pandas
matplotlib

