# 🧠 Learning in Layers: Multi-Layer Perceptron Classifier

This project demonstrates the implementation of a **Multi-Layer Perceptron (MLP)** — a type of feedforward artificial neural network — for classification.

🧪 This project uses a simple MLP model to classify structured data based on multiple input features.

---

## 🧠 What is a Multi-Layer Perceptron?

An MLP consists of an input layer, one or more **hidden layers**, and an output layer. Each layer contains neurons that apply weighted sums and activation functions to learn patterns in data.

MLPs are the foundation of:
- Deep learning
- Neural networks used in image/text classification
- Recommender systems and more

---

## 🚀 What This Project Covers

- Implementing MLP with scikit-learn's `MLPClassifier`
- Normalizing input features
- Training and testing the model
- Evaluating performance using:
  - Accuracy
  - Precision, Recall, F1 Score
  - Confusion matrix
- Analyzing convergence behavior and learning curves

---

## 📊 Results Summary

> ✅ Achieved strong classification performance using ReLU activation  
> ✅ Model trained successfully with tuned hyperparameters (hidden layer size, alpha)  
> ✅ Balanced classification metrics and fast convergence

---

## 🗂️ Files Included

- `MLP.py` — MLP implementation using `MLPClassifier`  
- `MLP_Results.pdf` — Report with results and discussion

---

## 📚 Technologies & Libraries

```bash
scikit-learn
numpy
pandas
matplotlib

