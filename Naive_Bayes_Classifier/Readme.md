# 📨 Filtering by Probability: A Naive Bayes Classification Project

This project explores **Naive Bayes Classification**, a probabilistic machine learning algorithm based on Bayes' Theorem. It’s known for its simplicity, speed, and surprising performance — especially on text and categorical data.

🧪 Completed as part of AIT 636 (Machine Learning) at George Mason University, this project implements a Gaussian Naive Bayes classifier to predict categorical outcomes from structured numerical features.

---

## 🧠 What is Naive Bayes?

Naive Bayes uses conditional probability to predict the class of a given data point. It assumes that the features are **independent**, which simplifies computation and makes it extremely fast.

It’s commonly used for:
- Spam filtering
- Text classification
- Sentiment analysis
- Medical diagnosis

---

## 🚀 What This Project Covers

- Implementing a **Gaussian Naive Bayes** model using `scikit-learn`
- Splitting data into training and test sets
- Predicting class labels on unseen data
- Evaluating model using:
  - Accuracy
  - Confusion matrix
  - Precision, recall, and F1 score
- Interpreting results in a structured report

---

## 📊 Results Summary

> ✅ Model achieved strong classification performance on the target dataset  
> ✅ Showed balanced precision and recall  
> ✅ Confirmed assumptions about feature independence and normality for Gaussian NB

---

## 🗂️ Files Included

- `Naive_Bayes.py` — Main Naive Bayes implementation  
- `Naive_Bayes_Results.pdf` — Report with metrics and evaluation  

---

## 📚 Technologies & Libraries

```bash
scikit-learn
pandas
numpy
matplotlib

