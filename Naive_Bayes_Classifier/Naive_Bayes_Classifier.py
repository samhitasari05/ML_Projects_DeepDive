# -*- coding: utf-8 -*-
"""AIT_636_5B.py.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15CHKQMWHXHHDhuBDqAKeMlCyW7HCyxbZ
"""

from google.colab import drive
drive.mount('/content/drive')

/#content/pima-indians-diabetes.csv

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn import metrics

df = pd.read_csv('/content/drive/My Drive/AIT_636_3rd sem/pima-indians-diabetes.csv', index_col=0)

feature_names = df.columns[:-1]
print(df.head())

# Standardize the features
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(df.drop('target', axis=1))
StandardScaler(copy=True, with_mean=True, with_std=True)
scaled_features = scaler.transform(df.drop('target', axis=1))
df_feat = pd.DataFrame(scaled_features, columns=df.columns[:-1])
print(df_feat.head())

import seaborn as sns
sns.pairplot(df, hue='target')
plt.show()

# Split the data into train and test
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(scaled_features, df['target'], test_size=0.3, stratify=df['target'], random_state=42)

from sklearn.naive_bayes import GaussianNB

clf = GaussianNB(priors=None) # An array whose size is equal to the number of classes. If not specified or None, the priors are adjusted based on relative class frequencies. Set it to [0.5 0.5] if you want equal priors.

clf = clf.fit(x_train, y_train)

print('Class priors are: ', clf.class_prior_)

# Predictions
predictions_test = clf.predict(x_test)
class_names = [0, 1]
predictions_test[predictions_test <= 0.5] = 0
predictions_test[predictions_test > 0.5] = 1
predictions_test = predictions_test.astype(int)

# Display confusion matrix
confusion_matrix = metrics.confusion_matrix(y_test, predictions_test, labels=class_names)
confusion_matrix_display = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=class_names)
confusion_matrix_display.plot()
plt.show()

# Report Overall Accuracy, precision, recall, F1-score
print(metrics.classification_report(
    y_true=y_test,
    y_pred=predictions_test,
    target_names=list(map(str, class_names)),
    zero_division=0 # Whenever number is divided by zero, instead of nan, return 0
))

# Hyperparameter Optimization
# Measuring the cross-validation accuracy for different values of hyperparameter and choosing the hyperparameter that results in the highest accuracy
from sklearn.model_selection import cross_val_score
cross_validation_accuracies = []
overall_accuracies = []
cross_validation_precisions = []
cross_validation_recalls = []
cross_validation_f1scores = []
cross_validation_roc_auc = []
priors = ([0,1],[0.1,0.9],[0.2,0.8],[0.3,0.7],[0.4,0.6],[0.5,0.5],[0.6,0.4],[0.7,0.3],[0.8,0.2],[0.9,0.1],[1,0])
for p in priors:
    print('Priors are:', p)

    clf = clf = GaussianNB(priors=p)
    clf = clf.fit(x_train, y_train)
    predictions_test = clf.predict(x_test)

    print(metrics.classification_report(
    y_true=y_test,
    y_pred=predictions_test,
    target_names=list(map(str, class_names)),
    zero_division=0 # Whenever number is divided by zero, instead of nan, return 0
    ))
    overall_accuracies.append(metrics.accuracy_score(y_true=y_test, y_pred=predictions_test))


# Plot the Accuracy vs. Prior
plt.figure(figsize=(10, 6))
plt.plot(
    ['0-100', '10-90', '20-80', '30-70', '40-60', '50-50', '60-40', '70-30', '80-20', '90-10','100-0'],
    overall_accuracies, color='blue', linestyle='dashed', marker='o', markerfacecolor='red', markersize=10
)
plt.title('Accuracy vs. Prior')
plt.xlabel('Prior Distribution')
plt.ylabel('Accuracy')
plt.show()