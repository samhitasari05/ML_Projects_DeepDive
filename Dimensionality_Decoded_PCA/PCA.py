# -*- coding: utf-8 -*-
"""AIT_636_Assignment9B_PCA_SamhitaSari.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FIb6TQtCnSwRJrwoMEzmoZpTsmHx4Oki
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn import metrics

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn import linear_model

# Load the dataset
dataset = pd.read_csv('/content/drive/My Drive/AIT_636_3rd sem/pima-indians-diabetes.csv', index_col=0)
columns = dataset.columns[:-1]

# Standardize the features
scaler = StandardScaler()
scaler.fit(dataset.drop('target', axis=1))
scaled_data = scaler.transform(dataset.drop('target', axis=1))
scaled_df = pd.DataFrame(scaled_data, columns=columns)

# Splitting the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(scaled_data, dataset['target'], test_size=0.3, stratify=dataset['target'], random_state=42)

# Apply PCA to reduce dimensionality to 2 components
pca_model = PCA(n_components=2)
pca_model.fit(X_train)
X_train_pca = pca_model.transform(X_train)
X_test_pca = pca_model.transform(X_test)

# Model Selection and Training
# We have to Uncomment one of these lines to try different models and also change the kernels as per the requirement to get the necessary outputs
#model = SVC(C=1.0, kernel='sigmoid')
#model = DecisionTreeClassifier(max_depth=None, min_samples_split=4, min_samples_leaf=2)
#model = RandomForestClassifier(max_depth=4, min_samples_split=3, min_samples_leaf=2, n_estimators=40)
#model = LogisticRegression(C=2, random_state=42)
#model = GaussianNB()
#model = KNeighborsClassifier(n_neighbors=3)
#model = linear_model.LinearRegression()
#model = linear_model.Ridge(alpha=0.6, random_state=0)
model = linear_model.Lasso(alpha=0.1, random_state=0)
model.fit(X_train_pca, Y_train)

# Predict on the test set
predictions = model.predict(X_test_pca)
predictions[predictions <= 0.5] = 0
predictions[predictions > 0.5] = 1
predictions = predictions.astype(int)

# Scatter plot of training samples
Y_train_list = Y_train.tolist()
plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=Y_train_list, s=10, cmap='viridis')

# Define grid for decision boundary
dim1_min, dim1_max = X_train_pca[:, 0].min() - 1, X_train_pca[:, 0].max() + 1
dim2_min, dim2_max = X_train_pca[:, 1].min() - 1, X_train_pca[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(dim1_min, dim1_max, .01), np.arange(dim2_min, dim2_max, .01))

# Predict over the grid to create decision surface
Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
Z[Z <= 0.5] = 0
Z[Z > 0.5] = 1
Z = Z.astype(int)
Z = Z.reshape(xx.shape)

# Plot decision boundary
plt.contour(xx, yy, Z)
plt.title('Lasso Regression Decision Surface')
plt.axis('off')
plt.show()