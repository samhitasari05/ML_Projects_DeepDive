# -*- coding: utf-8 -*-
"""AIT_636_Assignment8B_RF_DT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18Pl-QItpgWTNb0_knkMyZi2BVF7QRkLM
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier

# Loading the dataset
dataset = pd.read_csv('/content/drive/My Drive/AIT_636_3rd sem/pima-indians-diabetes.csv', index_col=0)
feature_names = dataset.columns[:-1]

scaler = StandardScaler()
scaled_features = scaler.fit_transform(dataset.drop('target', axis=1))
dataset_feat = pd.DataFrame(scaled_features, columns=dataset.columns[:-1])
x_train, x_test, y_train, y_test = train_test_split(scaled_features, dataset['target'], test_size=0.3, stratify=dataset['target'], random_state=42)

# Training a Decision Tree model
from sklearn import tree

# Initialize the Decision Tree classifier with specific parameters
dt_clf = tree.DecisionTreeClassifier(
    max_depth=None,
    min_samples_split=6,  # Minimum number of samples required to split a node
    min_samples_leaf=2  # Minimum number of samples needed to form a leaf
)

# Fitting the Decision Tree to the training data
dt_clf = dt_clf.fit(x_train, y_train)

# Train a Random Forest model
from sklearn.ensemble import RandomForestClassifier

# Initialize the Random Forest classifier with specific parameters
rf_clf = RandomForestClassifier(
    max_depth=None,
    min_samples_split=7,  # Minimum samples needed to split a node
    min_samples_leaf=2,  # Minimum samples needed to form a leaf
    n_estimators=100  # Number of decision trees in the forest
)

# Fit the Random Forest model to the training data
rf_clf = rf_clf.fit(x_train, y_train)

# Generate a visualization of the Decision Tree structure
class_names = list(map(str, dt_clf.classes_))
plt.figure(figsize=(16, 8))

# Plotting the Decision Tree
tree.plot_tree(
    decision_tree=dt_clf,
    max_depth=3,  # Show only the first 3 levels of the tree
    feature_names=feature_names,  # Names of the features used in training
    class_names=class_names,  # Names of the classes in the target
    filled=True  # Fill nodes with colors to represent class predictions
)
plt.show()

# Defining the hyperparameters to experiment with
split_values = [5, 10, 15, 20]
leaf_values = [3, 7, 11, 15]

# Create a dictionary to store evaluation results
results = {'Decision Tree': [], 'Random Forest': []}

# Function to create and return a model based on the provided parameters
def create_model(model_type, split, leaf):
    if model_type == 'Decision Tree':
        return tree.DecisionTreeClassifier(
            max_depth=None,
            min_samples_split=split,
            min_samples_leaf=leaf,
            random_state=42
        )
    elif model_type == 'Random Forest':
        return RandomForestClassifier(
            max_depth=None,
            min_samples_split=split,
            min_samples_leaf=leaf,
            n_estimators=100,
            random_state=42
        )

# Train and evaluate models with different hyperparameters
for model_name in ['Decision Tree', 'Random Forest']:
    for split in split_values:  # Iterate over different split values
        for leaf in leaf_values:  # Iterate over different leaf values
            # Creating the model with the given parameters
            clf = create_model(model_name, split, leaf)

            # Evaluate the model using cross-validation and calculate metrics
            accuracy = cross_val_score(clf, scaled_features, dataset['target'], cv=10, scoring='accuracy').mean()
            precision = cross_val_score(clf, scaled_features, dataset['target'], cv=10, scoring='precision_weighted').mean()
            recall = cross_val_score(clf, scaled_features, dataset['target'], cv=10, scoring='recall_weighted').mean()
            f1 = cross_val_score(clf, scaled_features, dataset['target'], cv=10, scoring='f1_weighted').mean()

            # Store the evaluation metrics for each model and parameter set
            results[model_name].append({
                'min_samples_split': split,
                'min_samples_leaf': leaf,
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1
            })

            # Printing the performance metrics for this configuration
            print(f"{model_name} - split: {split}, leaf: {leaf} | "
                  f"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, "
                  f"Recall: {recall:.4f}, F1-Score: {f1:.4f}")

# Plotting the relationship between min_samples_leaf and accuracy

# Create a figure with two side-by-side plots
plt.figure(figsize=(12, 6))

# Plot accuracy for Decision Tree
plt.subplot(1, 2, 1)  # Subplot for Decision Tree
for split in split_values:
    # Extract accuracy scores for the current split value
    accuracy_scores = [res['accuracy'] for res in results['Decision Tree']
                       if res['min_samples_split'] == split]
    # Plot the accuracy vs min_samples_leaf
    plt.plot(leaf_values, accuracy_scores, marker='o', linestyle='-', label=f'split = {split}')

# Add labels and title for Decision Tree plot
plt.title('Decision Tree: Accuracy vs min_samples_leaf')
plt.xlabel('min_samples_leaf')
plt.ylabel('Accuracy')
plt.legend()

# Plot accuracy for Random Forest
plt.subplot(1, 2, 2)  # Subplot for Random Forest
for split in split_values:
    # Extract accuracy scores for the current split value
    accuracy_scores = [res['accuracy'] for res in results['Random Forest']
                       if res['min_samples_split'] == split]
    # Plot the accuracy vs min_samples_leaf
    plt.plot(leaf_values, accuracy_scores, marker='o', linestyle='-', label=f'split = {split}')

# Add labels and title for Random Forest plot
plt.title('Random Forest: Accuracy vs min_samples_leaf')
plt.xlabel('min_samples_leaf')
plt.ylabel('Accuracy')
plt.legend()

# Adjusting layout to prevent overlap and display the plots
plt.tight_layout()
plt.show()