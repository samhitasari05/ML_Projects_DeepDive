# ğŸŒ² From Rules to Forests: Decision Tree & Random Forest Classifiers

This project showcases two of the most widely used tree-based classification models â€” **Decision Trees** and **Random Forests**.

ğŸ§ª Developed as part of AIT 636 (Machine Learning) at George Mason University, this project compares the performance of a single Decision Tree with an ensemble of trees (Random Forest) on a binary classification task.

---

## ğŸŒ³ What Are Tree-Based Models?

- **Decision Trees** create rule-based splits of data to classify outcomes. They're simple, interpretable, and fast â€” but prone to overfitting.
- **Random Forests** are ensembles of multiple Decision Trees trained on random subsets of data and features. They provide **better generalization** and **more robust predictions**.

These models are used in:
- Credit scoring
- Fraud detection
- Medical diagnosis
- Risk analysis

---

## ğŸš€ What This Project Covers

- Implementing Decision Tree & Random Forest using `scikit-learn`
- Visualizing decision trees and feature importances
- Evaluating models using:
  - Accuracy
  - Precision, Recall, F1-Score
  - Confusion Matrix
- Comparing performance of both models

---

## ğŸ“Š Results Summary

> âœ… Random Forest outperformed single Decision Tree across all metrics  
> âœ… Ensemble method helped reduce variance and improved generalization  
> âœ… Feature importance plot provided intuitive insights into the model's decisions

*(Optional: include `feature_importance_plot.png` and `confusion_matrix_rf.png`)*

---

## ğŸ—‚ï¸ Files Included

- `rf_dt.py` â€” Python script with DT & RF implementations  
- `rf_dt_Results.pdf` â€” Full write-up with results, evaluation, and visuals

---

## ğŸ“š Technologies & Libraries

```bash
scikit-learn
numpy
pandas
matplotlib

