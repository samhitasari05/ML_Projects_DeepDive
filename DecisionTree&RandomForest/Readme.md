# 🌲 From Rules to Forests: Decision Tree & Random Forest Classifiers

This project showcases two of the most widely used tree-based classification models — **Decision Trees** and **Random Forests**.

🧪 Developed as part of AIT 636 (Machine Learning) at George Mason University, this project compares the performance of a single Decision Tree with an ensemble of trees (Random Forest) on a binary classification task.

---

## 🌳 What Are Tree-Based Models?

- **Decision Trees** create rule-based splits of data to classify outcomes. They're simple, interpretable, and fast — but prone to overfitting.
- **Random Forests** are ensembles of multiple Decision Trees trained on random subsets of data and features. They provide **better generalization** and **more robust predictions**.

These models are used in:
- Credit scoring
- Fraud detection
- Medical diagnosis
- Risk analysis

---

## 🚀 What This Project Covers

- Implementing Decision Tree & Random Forest using `scikit-learn`
- Visualizing decision trees and feature importances
- Evaluating models using:
  - Accuracy
  - Precision, Recall, F1-Score
  - Confusion Matrix
- Comparing performance of both models

---

## 📊 Results Summary

> ✅ Random Forest outperformed single Decision Tree across all metrics  
> ✅ Ensemble method helped reduce variance and improved generalization  
> ✅ Feature importance plot provided intuitive insights into the model's decisions

*(Optional: include `feature_importance_plot.png` and `confusion_matrix_rf.png`)*

---

## 🗂️ Files Included

- `rf_dt.py` — Python script with DT & RF implementations  
- `rf_dt_Results.pdf` — Full write-up with results, evaluation, and visuals

---

## 📚 Technologies & Libraries

```bash
scikit-learn
numpy
pandas
matplotlib

